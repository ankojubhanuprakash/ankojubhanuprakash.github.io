{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to MkDocs For full documentation visit mkdocs.org . Commands mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit. Project layout mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Welcome to MkDocs"},{"location":"#welcome-to-mkdocs","text":"For full documentation visit mkdocs.org .","title":"Welcome to MkDocs"},{"location":"#commands","text":"mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit.","title":"Commands"},{"location":"#project-layout","text":"mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Project layout"},{"location":"NLP/Encode_Text_Data/","text":"Encode Text Data Bag of Words Bag of words is method to represent raw text data in a strucutred format to a machine learning model. Bag of words method constructs Vocabulary from the given corpus, and represents the text data as a vecotor of size of the constructed vocabulary, with a number indicating the count of occurence of a word, that appeared in the sentecne. Example - 1. \"John likes to watch movies. Mary likes movies too.\" 2. \"Mary also likes to watch football games.\" Vocabulary - \"John\",\"likes\",\"to\",\"watch\",\"movies\",\"Mary\", \"too\", \"also\",\"football\",\"games\" Representation - \"John likes to watch movies. Mary likes movies too.\" is represented as [1, 2, 1, 1, 2, 1, 1, 0, 0, 0] \"Mary also likes to watch football games.\" is represented as [0, 1, 1, 1, 0, 1, 0, 1, 1, 1] Limitation - 1. As the vocabulary size increases, the lenght of vecotr increases, leading to increase in computational cost. 2. Context of the sentence is not captured, by ignoreing the word order Implementation - TF-IDF In bag of Words, we have seen that elements in the vecotr represents the number of times the word appeared in the sentence. Instead of count, TF-IDF is alternative way to construct vectors that yields better results. \"A problem with scoring word frequency is that highly frequent words start to dominate in the document (e.g. larger score), but may not contain as much \u201cinformational content\u201d to the model as rarer but perhaps domain specific words.\" (https://machinelearningmastery.com/gentle-introduction-bag-words-model/) Definition - Term Frequency: is a scoring of the frequency of the word in the current document. where ft,d is the raw count of a term in a document, i.e., the number of times that term t occurs in document d. Inverse Document Frequency: is a scoring of how rare the word is across documents. N: total number of documents in the corpus {\\displaystyle N={|D|}}N = {|D|} {\\displaystyle |\\{d\\in D:t\\in d\\}|} |\\{d \\in D: t \\in d\\}| : number of documents where the term {\\displaystyle t}t appears (i.e., {\\displaystyle \\mathrm {tf} (t,d)\\neq 0} \\mathrm{tf}(t,d) \\neq 0). If the term is not in the corpus, this will lead to a division-by-zero. It is therefore common to adjust the denominator to {\\displaystyle 1+|\\{d\\in D:t\\in d\\}|}1 + |\\{d \\in D: t \\in d\\}|. Then tf\u2013idf is calculated as Example - References - 1. https://en.wikipedia.org/wiki/Bag-of-words_model 2. https://machinelearningmastery.com/gentle-introduction-bag-words-model/","title":"Encode Text Data"},{"location":"NLP/Encode_Text_Data/#encode-text-data","text":"","title":"Encode Text Data"},{"location":"NLP/Encode_Text_Data/#bag-of-words","text":"Bag of words is method to represent raw text data in a strucutred format to a machine learning model. Bag of words method constructs Vocabulary from the given corpus, and represents the text data as a vecotor of size of the constructed vocabulary, with a number indicating the count of occurence of a word, that appeared in the sentecne. Example - 1. \"John likes to watch movies. Mary likes movies too.\" 2. \"Mary also likes to watch football games.\" Vocabulary - \"John\",\"likes\",\"to\",\"watch\",\"movies\",\"Mary\", \"too\", \"also\",\"football\",\"games\" Representation - \"John likes to watch movies. Mary likes movies too.\" is represented as [1, 2, 1, 1, 2, 1, 1, 0, 0, 0] \"Mary also likes to watch football games.\" is represented as [0, 1, 1, 1, 0, 1, 0, 1, 1, 1] Limitation - 1. As the vocabulary size increases, the lenght of vecotr increases, leading to increase in computational cost. 2. Context of the sentence is not captured, by ignoreing the word order Implementation -","title":"Bag of Words"},{"location":"NLP/Encode_Text_Data/#tf-idf","text":"In bag of Words, we have seen that elements in the vecotr represents the number of times the word appeared in the sentence. Instead of count, TF-IDF is alternative way to construct vectors that yields better results. \"A problem with scoring word frequency is that highly frequent words start to dominate in the document (e.g. larger score), but may not contain as much \u201cinformational content\u201d to the model as rarer but perhaps domain specific words.\" (https://machinelearningmastery.com/gentle-introduction-bag-words-model/) Definition - Term Frequency: is a scoring of the frequency of the word in the current document. where ft,d is the raw count of a term in a document, i.e., the number of times that term t occurs in document d. Inverse Document Frequency: is a scoring of how rare the word is across documents. N: total number of documents in the corpus {\\displaystyle N={|D|}}N = {|D|} {\\displaystyle |\\{d\\in D:t\\in d\\}|} |\\{d \\in D: t \\in d\\}| : number of documents where the term {\\displaystyle t}t appears (i.e., {\\displaystyle \\mathrm {tf} (t,d)\\neq 0} \\mathrm{tf}(t,d) \\neq 0). If the term is not in the corpus, this will lead to a division-by-zero. It is therefore common to adjust the denominator to {\\displaystyle 1+|\\{d\\in D:t\\in d\\}|}1 + |\\{d \\in D: t \\in d\\}|. Then tf\u2013idf is calculated as Example - References - 1. https://en.wikipedia.org/wiki/Bag-of-words_model 2. https://machinelearningmastery.com/gentle-introduction-bag-words-model/","title":"TF-IDF"},{"location":"Pre-Processing/Scaler/","text":"Scaling StandardScaler import pandas as pd from sklearn.preprocessing import StandardScaler scaler=StandardScaler() data = [i for i in range(1,31)] data.reshape(-1,2) x=pd.DataFrame(data) scaler.fit_transform(data) MinMaxScaler from sklearn.preprocessing import MinMaxScaler data = [[-1, 2], [-0.5, 6], [0, 10], [1, 18]] scaler = MinMaxScaler() print(scaler.fit(data)) MinMaxScaler() print(scaler.data_max_) [ 1. 18.] print(scaler.transform(data)) [[0. 0. ] [0.25 0.25] [0.5 0.5 ] [1. 1. ]] print(scaler.transform([[2, 2]])) [[1.5 0. ]] MaxAbsScaler from sklearn.preprocessing import MaxAbsScaler X = [[ 1., -1., 2.], [ 2., 0., 0.], [ 0., 1., -1.]] transformer = MaxAbsScaler().fit(X) transformer MaxAbsScaler() transformer.transform(X) output: array([[ 0.5, -1. , 1. ], [ 1. , 0. , 0. ], [ 0. , 1. , -0.5]]) RobustScaler","title":"Scaling"},{"location":"Pre-Processing/Scaler/#scaling","text":"","title":"Scaling"},{"location":"Pre-Processing/Scaler/#standardscaler","text":"import pandas as pd from sklearn.preprocessing import StandardScaler scaler=StandardScaler() data = [i for i in range(1,31)] data.reshape(-1,2) x=pd.DataFrame(data) scaler.fit_transform(data)","title":"StandardScaler"},{"location":"Pre-Processing/Scaler/#minmaxscaler","text":"from sklearn.preprocessing import MinMaxScaler data = [[-1, 2], [-0.5, 6], [0, 10], [1, 18]] scaler = MinMaxScaler() print(scaler.fit(data)) MinMaxScaler() print(scaler.data_max_) [ 1. 18.] print(scaler.transform(data)) [[0. 0. ] [0.25 0.25] [0.5 0.5 ] [1. 1. ]] print(scaler.transform([[2, 2]])) [[1.5 0. ]]","title":"MinMaxScaler"},{"location":"Pre-Processing/Scaler/#maxabsscaler","text":"from sklearn.preprocessing import MaxAbsScaler X = [[ 1., -1., 2.], [ 2., 0., 0.], [ 0., 1., -1.]] transformer = MaxAbsScaler().fit(X) transformer MaxAbsScaler() transformer.transform(X) output: array([[ 0.5, -1. , 1. ], [ 1. , 0. , 0. ], [ 0. , 1. , -0.5]])","title":"MaxAbsScaler"},{"location":"Pre-Processing/Scaler/#robustscaler","text":"","title":"RobustScaler"}]}