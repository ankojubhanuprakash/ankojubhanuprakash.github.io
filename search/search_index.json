{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to MkDocs For full documentation visit mkdocs.org . Commands mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit. Project layout mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Welcome to MkDocs"},{"location":"#welcome-to-mkdocs","text":"For full documentation visit mkdocs.org .","title":"Welcome to MkDocs"},{"location":"#commands","text":"mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit.","title":"Commands"},{"location":"#project-layout","text":"mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Project layout"},{"location":"Finance/Definitions/","text":"Secutity Collateral Debt Convertible Debt Non Convertible Debt Equity 7. Large - 1- 100 8. Mid : 100-250 9. Small - 250 - 500 10. Market captilization = price * quantity 11. free float 10 sensex = free float market captilisation / (base market capilisation * base index) Primary MArket IPO FPO rIGHTS issues Bonus Issue Secondary MArket Buying and seling of shares Terminology Valuation Face VAlue- provided by company Book VAlue or Total net worth = Total Assets - Total liabilities Replacement Value = assets value in cuurent market Enterprise value = Total equity(MArket captilizarion) + total debt - Cash reserve. More Robust than Market captilisation. Capital Market operations Share Split - Issues additional shares to existing customers, in order to increase number of shares in the market. Is done to make share more affordable. Share Consolidaion - opposite to Share split Buy Back - Company purchass shares from existing share holders via the stock market. Gaining back control. VAlua","title":"Definitions"},{"location":"Machine%20Learning/Pre-Processing/","text":"Scaling StandardScaler import pandas as pd from sklearn.preprocessing import StandardScaler scaler=StandardScaler() data = [i for i in range(1,31)] data.reshape(-1,2) x=pd.DataFrame(data) scaler.fit_transform(data) MinMaxScaler from sklearn.preprocessing import MinMaxScaler data = [[-1, 2], [-0.5, 6], [0, 10], [1, 18]] scaler = MinMaxScaler() print(scaler.fit(data)) MinMaxScaler() print(scaler.data_max_) [ 1. 18.] print(scaler.transform(data)) [[0. 0. ] [0.25 0.25] [0.5 0.5 ] [1. 1. ]] print(scaler.transform([[2, 2]])) [[1.5 0. ]] MaxAbsScaler from sklearn.preprocessing import MaxAbsScaler X = [[ 1., -1., 2.], [ 2., 0., 0.], [ 0., 1., -1.]] transformer = MaxAbsScaler().fit(X) transformer MaxAbsScaler() `transformer.transform(X)y array([[ 0.5, -1. , 1. ], [ 1. , 0. , 0. ], [ 0. , 1. , -0.5]]) RobustScaler","title":"Scaling"},{"location":"Machine%20Learning/Pre-Processing/#scaling","text":"","title":"Scaling"},{"location":"Machine%20Learning/Pre-Processing/#standardscaler","text":"import pandas as pd from sklearn.preprocessing import StandardScaler scaler=StandardScaler() data = [i for i in range(1,31)] data.reshape(-1,2) x=pd.DataFrame(data) scaler.fit_transform(data)","title":"StandardScaler"},{"location":"Machine%20Learning/Pre-Processing/#minmaxscaler","text":"from sklearn.preprocessing import MinMaxScaler data = [[-1, 2], [-0.5, 6], [0, 10], [1, 18]] scaler = MinMaxScaler() print(scaler.fit(data)) MinMaxScaler() print(scaler.data_max_) [ 1. 18.] print(scaler.transform(data)) [[0. 0. ] [0.25 0.25] [0.5 0.5 ] [1. 1. ]] print(scaler.transform([[2, 2]])) [[1.5 0. ]]","title":"MinMaxScaler"},{"location":"Machine%20Learning/Pre-Processing/#maxabsscaler","text":"from sklearn.preprocessing import MaxAbsScaler X = [[ 1., -1., 2.], [ 2., 0., 0.], [ 0., 1., -1.]] transformer = MaxAbsScaler().fit(X) transformer MaxAbsScaler() `transformer.transform(X)y array([[ 0.5, -1. , 1. ], [ 1. , 0. , 0. ], [ 0. , 1. , -0.5]])","title":"MaxAbsScaler"},{"location":"Machine%20Learning/Pre-Processing/#robustscaler","text":"","title":"RobustScaler"},{"location":"Machine%20Learning/Validation/","text":"Test train Split from sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split(df['feature'],df['Class'] , test_size=0.33, random_state=42) K Fold Cross validation Cros val score Cros val pred","title":"Validation"},{"location":"Mlops/Definitions/","text":"Concept Drift Data Drift Data Provenance Hamming Eucledian Manhattan chebyshey In two dimensions, i.e. plane geometry, if the points p and q have Cartesian coordinates {\\displaystyle (x_{1},y_{1})}(x_{1},y_{1}) and {\\displaystyle (x_{2},y_{2})}(x_{2},y_{2}), their Chebyshev distance is {\\displaystyle D_{\\rm {Chebyshev}}=\\max \\left(\\left|x_{2}-x_{1}\\right|,\\left|y_{2}-y_{1}\\right|\\right).}{\\displaystyle D_{\\rm {Chebyshev}}=\\max \\left(\\left|x_{2}-x_{1}\\right|,\\left|y_{2}-y_{1}\\right|\\right).} mahalanobis","title":"Definitions"},{"location":"Mlops/Error%20Analysis/","text":"Baselines Ameisen (2018) names four levels of performance we can categorize a model into. Using these levels of performance we can determine what level we want our model to achieve based on our domain knowledge, our computational and human resources, and our business goals. Each level of performance will guide our selection of a baseline. Trivially Attainable Performance The Trivially Attainable Performance is the performance that can be obtained easily and you should expect your model to be better than this. For instance, in classification, guessing the most frequent class provides a quick solution. Human Level Performance We use machine learning methods mainly due to two reasons: automate tasks, or perform tasks where machines and algorithms are faster and better than humans (AlphaGo). For the former, a model achieving Human Level Performance means that the model is ready for deployment and for the latter it is only a lower bound on the expected performance. Reasonable Automated Performance Reasonable Automated Performance represents what can be obtained with a relatively simple method. This benchmark is critical in assessing whether a complex model is performing well and in addressing the accuracy and complexity trade-off. Required Deployment Performance Finally, Required Deployment Performance is the minimal performance that makes your model ready for production from a business standpoint. Having constructed a baseline, the next step is to see what the baseline fails to capture. This will guide our choice of a more complex model. That being said, we should also not neglect the fact that improving upon baselines can make already successful cases fail, as improvements are not strictly additive. 1. For example, for a classification task we can get an idea of what classes are hard to separate from each other by looking at the confusion matrix. 2. If we have a lasso regression problem, we can look at the non-zero coefficients to guide feature selection. Furthermore, if the model neglects some features that our domain knowledge tells us are important, then that sends a warning signal as our data may not be a good representation of the population or it could just be that the model is not suitable. Improving Baselines Strategically These ideas were originally presented in Merity (2017). Do not add unnecessary complexity A good rule of thumb is to start with a simple baseline, for example, logistic regression. The baseline should be stable in the sense that its results should be warranted up to some degree. Therefore, following Occam\u2019s Razor principle is most valid for a baseline. If adding a lot of complexity means a tiny increase in performance, then keep that complexity out of the baseline. Find the limits of your baseline After we have constructed a simple and fast baseline we get it into shape. This can be done by: extensively tuning parameters, adopting regularization techniques, conducting data processing etc. We can afford many runs tuning the hyperparameters because the baseline is simple. Similarly, bugs and flawed assumptions become easier to find as the model is not complex enough to hide them from us. Gradually progress towards complex models While we progress towards more complex models, it is important to understand if the complexity we add is warranted. A strategy that people follow is to keep increasing the model\u2019s parameters until it overfits on the dataset and then introduce regularization to prevent overfitting. Even though some work (Belkin 2019) has shown that further increasing the complexity of your model may lead to good generalization, this still leads to a lack of understanding of why increasing complexity works. Besides, simply adding complexity leads to too many moving parts, and it may be hard to isolate the improvements. A key idea when moving towards more complex models is to make sure we know what we are trading off. Add a component, one at a time, evaluate it, and then keep it or remove it. For example, we may have to weigh between component A which makes the model far slower but achieves gains, or component B that makes the model less general but faster. Error Analysis interepretml, fairlearn, error analysis Experiment Tracking References Baselines Error Analysis Experiment Tracking","title":"Baselines"},{"location":"Mlops/Error%20Analysis/#baselines","text":"Ameisen (2018) names four levels of performance we can categorize a model into. Using these levels of performance we can determine what level we want our model to achieve based on our domain knowledge, our computational and human resources, and our business goals. Each level of performance will guide our selection of a baseline. Trivially Attainable Performance The Trivially Attainable Performance is the performance that can be obtained easily and you should expect your model to be better than this. For instance, in classification, guessing the most frequent class provides a quick solution. Human Level Performance We use machine learning methods mainly due to two reasons: automate tasks, or perform tasks where machines and algorithms are faster and better than humans (AlphaGo). For the former, a model achieving Human Level Performance means that the model is ready for deployment and for the latter it is only a lower bound on the expected performance. Reasonable Automated Performance Reasonable Automated Performance represents what can be obtained with a relatively simple method. This benchmark is critical in assessing whether a complex model is performing well and in addressing the accuracy and complexity trade-off. Required Deployment Performance Finally, Required Deployment Performance is the minimal performance that makes your model ready for production from a business standpoint. Having constructed a baseline, the next step is to see what the baseline fails to capture. This will guide our choice of a more complex model. That being said, we should also not neglect the fact that improving upon baselines can make already successful cases fail, as improvements are not strictly additive. 1. For example, for a classification task we can get an idea of what classes are hard to separate from each other by looking at the confusion matrix. 2. If we have a lasso regression problem, we can look at the non-zero coefficients to guide feature selection. Furthermore, if the model neglects some features that our domain knowledge tells us are important, then that sends a warning signal as our data may not be a good representation of the population or it could just be that the model is not suitable. Improving Baselines Strategically These ideas were originally presented in Merity (2017). Do not add unnecessary complexity A good rule of thumb is to start with a simple baseline, for example, logistic regression. The baseline should be stable in the sense that its results should be warranted up to some degree. Therefore, following Occam\u2019s Razor principle is most valid for a baseline. If adding a lot of complexity means a tiny increase in performance, then keep that complexity out of the baseline. Find the limits of your baseline After we have constructed a simple and fast baseline we get it into shape. This can be done by: extensively tuning parameters, adopting regularization techniques, conducting data processing etc. We can afford many runs tuning the hyperparameters because the baseline is simple. Similarly, bugs and flawed assumptions become easier to find as the model is not complex enough to hide them from us. Gradually progress towards complex models While we progress towards more complex models, it is important to understand if the complexity we add is warranted. A strategy that people follow is to keep increasing the model\u2019s parameters until it overfits on the dataset and then introduce regularization to prevent overfitting. Even though some work (Belkin 2019) has shown that further increasing the complexity of your model may lead to good generalization, this still leads to a lack of understanding of why increasing complexity works. Besides, simply adding complexity leads to too many moving parts, and it may be hard to isolate the improvements. A key idea when moving towards more complex models is to make sure we know what we are trading off. Add a component, one at a time, evaluate it, and then keep it or remove it. For example, we may have to weigh between component A which makes the model far slower but achieves gains, or component B that makes the model less general but faster.","title":"Baselines"},{"location":"Mlops/Error%20Analysis/#error-analysis","text":"interepretml, fairlearn, error analysis","title":"Error Analysis"},{"location":"Mlops/Error%20Analysis/#experiment-tracking","text":"","title":"Experiment Tracking"},{"location":"Mlops/Error%20Analysis/#references","text":"Baselines Error Analysis Experiment Tracking","title":"References"},{"location":"Mlops/Monitor_ml_model/","text":"References [https://christophergs.com/machine%20learning/2020/03/14/how-to-monitor-machine-learning-models/] ( https://christophergs.com/machine%20learning/2020/03/14/how-to-monitor-machine-learning-models/ )","title":"References"},{"location":"Mlops/Monitor_ml_model/#references","text":"[https://christophergs.com/machine%20learning/2020/03/14/how-to-monitor-machine-learning-models/] ( https://christophergs.com/machine%20learning/2020/03/14/how-to-monitor-machine-learning-models/ )","title":"References"},{"location":"Mlops/Tools/","text":"What If Tool TFDV interepretml, fairlearn, error analysis seed bank call back in keras, kera_fit_generator - course-1 6.https://github.com/https-deeplearning-ai/tensorflow-1-public/blob/main/C2/W1/ungraded_lab/C2_W1_Lab_1_cats_vs_dogs.ipynb -course-2 https://colab.sandbox.google.com/github/lmoroney/dlaicourse/blob/master/Course%202%20-%20Part%202%20-%20Lesson%202%20-%20Notebook.ipynb Why is overfitting more likely to occur on smaller datasets?","title":"Tools"},{"location":"NLP/DeepLearning/","text":"Introduction Natural Language processing applications do not follow the ssumption of independently and identically distributed (i.i.d.). For example, the words in sentence are arranged in a meaningful way. If we disturb the order the meaning changes, Why? each word is dependent on the previous word. So we need a different architecture that captures temporal structure in sentence. The same is case with the time series data, where each data point is also a function of its previous satapoint. we shall look at various instances architecture like RNN(Recuurent neural network), LSTM(Long Short Term Memory) and GRU (Gated recurrent Unit) that captures temporal context for a time series problem. RNN Like Convolutional Neural Networks share parameters across space, RNNs share parameters over time. The following example predicting next character based on previous character is demonstrated in below images. So for every input x_t we are calculating context h_t and output computed based on h_t and h_t-1 , where h_t-1 is computed from previous observatopn x_t-1 . So variable h holds the context of the sequence. An RNN function is implemented as {h_t=f(x_t,h_t-1)} h_t= \\phi(Ux + Vh_t-1 + b_h) \\hat{y}= g(Wh_t + b_y) Where U,V and W are the weight matrices to compute conetxt variable h and output y . Pytorch RNN cell Parameters - * input_size \u2013 The number of expected features in the input x. * hidden_size \u2013 The number of features in the hidden state h. * num_layers \u2013 Number of recurrent layers. E.g., setting num_layers=2 would mean stacking two RNNs together to form a stacked RNN, with the second RNN taking in outputs of the first RNN and computing the final results. Default: 1 Inputs : input, h_0 * input : tensor of shape (L, N, H_{in} ) when batch_first=False or (N, L, H_{in} ) when batch_first=True containing the features of the input sequence. * h_0 : tensor of shape (D * \\text{num\\_layers}, N, H_{out}) containing the initial hidden state for each element in the batch. Defaults to zeros if not provided. Where N= batch size \\\\ L= sequence length \\\\ D= 2\\ if\\ bidirectional\\ =\\ True\\ otherwise\\ 1 \\\\ H_{in} = \\text{input\\_size} \\\\ H_{out} = \\text{hidden\\_size} \u200b Outputs : output, h_n * output : tensor of shape (L, N, D H_{out} ) when batch_first=False or (N, L, D*H_{out} ) when batch_first=True containing the output features (h_t) from the last layer of the RNN, for each t. h_n: tensor of shape (D * \\text{num\\_layers}, N, H_{out}) containing the final hidden state for each element in the batch. import torch from torch import nn rnn = nn.RNN(3,4) print(torch.randn(3,5)) input=torch.randn(5,2,3) print(input) output,final_hidden=rnn(input) print(output.shape,final_hidden.shape) # input is (5,2,3) # rnn cell is nn.RNN(3,4) # output is (5,2,4) & final hidden activation is (1,2,4) Back Propogation through time LSTM GRU Time Series problem References https://d2l.ai/chapter_recurrent-neural-networks/index.html https://deeplearning.neuromatch.io/tutorials/W2D3_ModernRecurrentNeuralNetworks/student/W2D3_Tutorial2.html https://pytorch.org/docs/stable/generated/torch.nn.RNN.html","title":"Introduction"},{"location":"NLP/DeepLearning/#introduction","text":"Natural Language processing applications do not follow the ssumption of independently and identically distributed (i.i.d.). For example, the words in sentence are arranged in a meaningful way. If we disturb the order the meaning changes, Why? each word is dependent on the previous word. So we need a different architecture that captures temporal structure in sentence. The same is case with the time series data, where each data point is also a function of its previous satapoint. we shall look at various instances architecture like RNN(Recuurent neural network), LSTM(Long Short Term Memory) and GRU (Gated recurrent Unit) that captures temporal context for a time series problem.","title":"Introduction"},{"location":"NLP/DeepLearning/#rnn","text":"Like Convolutional Neural Networks share parameters across space, RNNs share parameters over time. The following example predicting next character based on previous character is demonstrated in below images. So for every input x_t we are calculating context h_t and output computed based on h_t and h_t-1 , where h_t-1 is computed from previous observatopn x_t-1 . So variable h holds the context of the sequence. An RNN function is implemented as {h_t=f(x_t,h_t-1)} h_t= \\phi(Ux + Vh_t-1 + b_h) \\hat{y}= g(Wh_t + b_y) Where U,V and W are the weight matrices to compute conetxt variable h and output y .","title":"RNN"},{"location":"NLP/DeepLearning/#pytorch-rnn-cell","text":"Parameters - * input_size \u2013 The number of expected features in the input x. * hidden_size \u2013 The number of features in the hidden state h. * num_layers \u2013 Number of recurrent layers. E.g., setting num_layers=2 would mean stacking two RNNs together to form a stacked RNN, with the second RNN taking in outputs of the first RNN and computing the final results. Default: 1 Inputs : input, h_0 * input : tensor of shape (L, N, H_{in} ) when batch_first=False or (N, L, H_{in} ) when batch_first=True containing the features of the input sequence. * h_0 : tensor of shape (D * \\text{num\\_layers}, N, H_{out}) containing the initial hidden state for each element in the batch. Defaults to zeros if not provided. Where N= batch size \\\\ L= sequence length \\\\ D= 2\\ if\\ bidirectional\\ =\\ True\\ otherwise\\ 1 \\\\ H_{in} = \\text{input\\_size} \\\\ H_{out} = \\text{hidden\\_size} \u200b Outputs : output, h_n * output : tensor of shape (L, N, D H_{out} ) when batch_first=False or (N, L, D*H_{out} ) when batch_first=True containing the output features (h_t) from the last layer of the RNN, for each t. h_n: tensor of shape (D * \\text{num\\_layers}, N, H_{out}) containing the final hidden state for each element in the batch. import torch from torch import nn rnn = nn.RNN(3,4) print(torch.randn(3,5)) input=torch.randn(5,2,3) print(input) output,final_hidden=rnn(input) print(output.shape,final_hidden.shape) # input is (5,2,3) # rnn cell is nn.RNN(3,4) # output is (5,2,4) & final hidden activation is (1,2,4)","title":"Pytorch RNN cell"},{"location":"NLP/DeepLearning/#back-propogation-through-time","text":"","title":"Back Propogation through time"},{"location":"NLP/DeepLearning/#lstm","text":"","title":"LSTM"},{"location":"NLP/DeepLearning/#gru","text":"","title":"GRU"},{"location":"NLP/DeepLearning/#time-series-problem","text":"","title":"Time Series problem"},{"location":"NLP/DeepLearning/#references","text":"https://d2l.ai/chapter_recurrent-neural-networks/index.html https://deeplearning.neuromatch.io/tutorials/W2D3_ModernRecurrentNeuralNetworks/student/W2D3_Tutorial2.html https://pytorch.org/docs/stable/generated/torch.nn.RNN.html","title":"References"},{"location":"NLP/Encode_Text_Data/","text":"Encode Text Data Bag of Words Bag of words is method to represent raw text data in a strucutred format to a machine learning model. Bag of words method constructs Vocabulary from the given corpus, and represents the text data as a vecotor of size of the constructed vocabulary, with a number indicating the count of occurence of a word, that appeared in the sentecne. Example - 1. \"John likes to watch movies. Mary likes movies too.\" 2. \"Mary also likes to watch football games.\" Vocabulary - \"John\",\"likes\",\"to\",\"watch\",\"movies\",\"Mary\", \"too\", \"also\",\"football\",\"games\" Representation - \"John likes to watch movies. Mary likes movies too.\" is represented as [1, 2, 1, 1, 2, 1, 1, 0, 0, 0] \"Mary also likes to watch football games.\" is represented as [0, 1, 1, 1, 0, 1, 0, 1, 1, 1] Limitation - 1. As the vocabulary size increases, the lenght of vecotr increases, leading to increase in computational cost. 2. Context of the sentence is not captured, by ignoreing the word order Implementation - TF-IDF In bag of Words, we have seen that elements in the vecotr represents the number of times the word appeared in the sentence. Instead of count, TF-IDF is alternative way to construct vectors that yields better results. \"A problem with scoring word frequency is that highly frequent words start to dominate in the document (e.g. larger score), but may not contain as much \u201cinformational content\u201d to the model as rarer but perhaps domain specific words.\" (https://machinelearningmastery.com/gentle-introduction-bag-words-model/) Definition - Term Frequency: is a scoring of the frequency of the word in the current document. where ft,d is the raw count of a term in a document, i.e., the number of times that term t occurs in document d. Inverse Document Frequency: is a scoring of how rare the word is across documents. N: total number of documents in the corpus. {\\displaystyle N={|D|}}N = {|D|} {\\displaystyle |\\{d\\in D:t\\in d\\}|} |\\{d \\in D: t \\in d\\}| : number of documents where the term {\\displaystyle t}t appears (i.e., {\\displaystyle \\mathrm {tf} (t,d)\\neq 0} \\mathrm{tf}(t,d) \\neq 0). If the term is not in the corpus, this will lead to a division-by-zero. It is therefore common to adjust the denominator to {\\displaystyle 1+|\\{d\\in D:t\\in d\\}|}1 + |\\{d \\in D: t \\in d\\}|. Then tf\u2013idf is calculated as Example - References - 1. https://en.wikipedia.org/wiki/Bag-of-words_model 2. https://machinelearningmastery.com/gentle-introduction-bag-words-model/","title":"Encode Text Data"},{"location":"NLP/Encode_Text_Data/#encode-text-data","text":"","title":"Encode Text Data"},{"location":"NLP/Encode_Text_Data/#bag-of-words","text":"Bag of words is method to represent raw text data in a strucutred format to a machine learning model. Bag of words method constructs Vocabulary from the given corpus, and represents the text data as a vecotor of size of the constructed vocabulary, with a number indicating the count of occurence of a word, that appeared in the sentecne. Example - 1. \"John likes to watch movies. Mary likes movies too.\" 2. \"Mary also likes to watch football games.\" Vocabulary - \"John\",\"likes\",\"to\",\"watch\",\"movies\",\"Mary\", \"too\", \"also\",\"football\",\"games\" Representation - \"John likes to watch movies. Mary likes movies too.\" is represented as [1, 2, 1, 1, 2, 1, 1, 0, 0, 0] \"Mary also likes to watch football games.\" is represented as [0, 1, 1, 1, 0, 1, 0, 1, 1, 1] Limitation - 1. As the vocabulary size increases, the lenght of vecotr increases, leading to increase in computational cost. 2. Context of the sentence is not captured, by ignoreing the word order Implementation -","title":"Bag of Words"},{"location":"NLP/Encode_Text_Data/#tf-idf","text":"In bag of Words, we have seen that elements in the vecotr represents the number of times the word appeared in the sentence. Instead of count, TF-IDF is alternative way to construct vectors that yields better results. \"A problem with scoring word frequency is that highly frequent words start to dominate in the document (e.g. larger score), but may not contain as much \u201cinformational content\u201d to the model as rarer but perhaps domain specific words.\" (https://machinelearningmastery.com/gentle-introduction-bag-words-model/) Definition - Term Frequency: is a scoring of the frequency of the word in the current document. where ft,d is the raw count of a term in a document, i.e., the number of times that term t occurs in document d. Inverse Document Frequency: is a scoring of how rare the word is across documents. N: total number of documents in the corpus. {\\displaystyle N={|D|}}N = {|D|} {\\displaystyle |\\{d\\in D:t\\in d\\}|} |\\{d \\in D: t \\in d\\}| : number of documents where the term {\\displaystyle t}t appears (i.e., {\\displaystyle \\mathrm {tf} (t,d)\\neq 0} \\mathrm{tf}(t,d) \\neq 0). If the term is not in the corpus, this will lead to a division-by-zero. It is therefore common to adjust the denominator to {\\displaystyle 1+|\\{d\\in D:t\\in d\\}|}1 + |\\{d \\in D: t \\in d\\}|. Then tf\u2013idf is calculated as Example - References - 1. https://en.wikipedia.org/wiki/Bag-of-words_model 2. https://machinelearningmastery.com/gentle-introduction-bag-words-model/","title":"TF-IDF"},{"location":"Python/Linear%20DataStructures%20basics/","text":"Linear Data Structures Here we are looking at basic datastructures. Arrays Basic Operations The basic operations supported by an array are as stated below \u2212 1.Traverse \u2212 print all the array elements one by one. 2.Insertion \u2212 Adds an element at the given index. 3.Deletion \u2212 Deletes an element at the given index. 4.Search \u2212 Searches an element using the given index or by the value. 5.Update \u2212 Updates an element at the given index. List Basic List Operations Length Concatenation Repetition ['Hi']*2 will give ['Hi','Hi'] Membership 1 in [1,2] will give True Delete List Element list1 = ['physics', 'chemistry', 1997, 2000] \\n del list1[2] Dictionary Keys must be immutable. Which means you can use strings, numbers or tuples as dictionary keys but something like ['key'] is not allowed. Matrix Matrix is created uing numoy array, adding a column and deleting a column is tricky come back and learn. Adding a Column We can add column to a matrix using the insert() method. here we have to mention the index where we want to add the column and a array containing the new values of the columns added.In the below example we add t a new column at the fifth position from the beginning. from numpy import * m = array([['Mon',18,20,22,17],['Tue',11,18,21,18], ['Wed',15,21,20,19],['Thu',11,20,22,21], ['Fri',18,17,23,22],['Sat',12,22,20,18], ['Sun',13,15,19,16]]) m_c = insert(m,[5],[[1],[2],[3],[4],[5],[6],[7]],1) Sets Mathematically a set is a collection of items not in any particular order. A Python set is similar to this mathematical definition with below additional conditions. The elements in the set cannot be duplicates. The elements in the set are immutable(cannot be modified) but the set as a whole is mutable. There is no index attached to any element in a python set. So they do not support any indexing or slicing operation. A set is created by using the set() function or placing all the elements within a pair of curly braces. Days=set([\"Mon\",\"Tue\",\"Wed\",\"Thu\",\"Fri\",\"Sat\",\"Sun\"]) Months={\"Jan\",\"Feb\",\"Mar\"}","title":"Linear Data Structures"},{"location":"Python/Linear%20DataStructures%20basics/#linear-data-structures","text":"Here we are looking at basic datastructures.","title":"Linear Data Structures"},{"location":"Python/Linear%20DataStructures%20basics/#arrays","text":"","title":"Arrays"},{"location":"Python/Linear%20DataStructures%20basics/#basic-operations","text":"The basic operations supported by an array are as stated below \u2212 1.Traverse \u2212 print all the array elements one by one. 2.Insertion \u2212 Adds an element at the given index. 3.Deletion \u2212 Deletes an element at the given index. 4.Search \u2212 Searches an element using the given index or by the value. 5.Update \u2212 Updates an element at the given index.","title":"Basic Operations"},{"location":"Python/Linear%20DataStructures%20basics/#list","text":"","title":"List"},{"location":"Python/Linear%20DataStructures%20basics/#basic-list-operations","text":"Length Concatenation Repetition ['Hi']*2 will give ['Hi','Hi'] Membership 1 in [1,2] will give True Delete List Element list1 = ['physics', 'chemistry', 1997, 2000] \\n del list1[2]","title":"Basic List Operations"},{"location":"Python/Linear%20DataStructures%20basics/#dictionary","text":"Keys must be immutable. Which means you can use strings, numbers or tuples as dictionary keys but something like ['key'] is not allowed.","title":"Dictionary"},{"location":"Python/Linear%20DataStructures%20basics/#matrix","text":"Matrix is created uing numoy array, adding a column and deleting a column is tricky come back and learn.","title":"Matrix"},{"location":"Python/Linear%20DataStructures%20basics/#adding-a-column","text":"We can add column to a matrix using the insert() method. here we have to mention the index where we want to add the column and a array containing the new values of the columns added.In the below example we add t a new column at the fifth position from the beginning. from numpy import * m = array([['Mon',18,20,22,17],['Tue',11,18,21,18], ['Wed',15,21,20,19],['Thu',11,20,22,21], ['Fri',18,17,23,22],['Sat',12,22,20,18], ['Sun',13,15,19,16]]) m_c = insert(m,[5],[[1],[2],[3],[4],[5],[6],[7]],1)","title":"Adding a Column"},{"location":"Python/Linear%20DataStructures%20basics/#sets","text":"Mathematically a set is a collection of items not in any particular order. A Python set is similar to this mathematical definition with below additional conditions. The elements in the set cannot be duplicates. The elements in the set are immutable(cannot be modified) but the set as a whole is mutable. There is no index attached to any element in a python set. So they do not support any indexing or slicing operation. A set is created by using the set() function or placing all the elements within a pair of curly braces. Days=set([\"Mon\",\"Tue\",\"Wed\",\"Thu\",\"Fri\",\"Sat\",\"Sun\"]) Months={\"Jan\",\"Feb\",\"Mar\"}","title":"Sets"},{"location":"Python/Linear%20DataStructures/","text":"Liner Data Structures","title":"Liner Data Structures"},{"location":"Python/Linear%20DataStructures/#liner-data-structures","text":"","title":"Liner Data Structures"},{"location":"Python/Linked_list/","text":"Linked List Create Node class with variables data and next_node Create Linked list Class, with variable name header. Create some node object and assign it to linked list head value. Create following functions. INsert Node at Head Insert Node at Tail Insert Node at some position Delete Node at head and tail Get length ofLinked List class Node: def __init__(self,data): self.data=data self.nxt_node = None class S_Lnkd_Lst: def __init__(self,): self.head = None def __len__(self): lcl_len=0 lcl_node=self.head while lcl_node is not None: lcl_len+=1 lcl_node=lcl_node.nxt_node return lcl_len def dsply_lst(self): tempnode=self.head while tempnode is not None: print(tempnode.data) tempnode=tempnode.nxt_node def add_data(self,position,data): lcl_node=Node(data) if position == 'head' : lcl_node.nxt_node = self.head self.head=lcl_node elif position == 'tail': if self.head is None: self.head=lcl_node return temp_node=self.head while temp_node.nxt_node is not None: temp_node=temp_node.nxt_node temp_node.nxt_node=lcl_node elif isinstance(position,Node): lcl_node.nxt_node=position.nxt_node position.nxt_node = lcl_node elif isinstance(position,int):# insert at some position if position > len(self) : print('CAnnot be insterted at {}',position) else: print('nothing') def rmv_node(self,position): if position is 'head': self.head=self.head.nxt_node if position is 'tail': if self.head is None: print('nothing to remove') return if len(self)==1: self.head=None return temp_node = self.head while temp_node.nxt_node is not None: prev=temp_node temp_node=temp_node.nxt_node prev.nxt_node=None lcl_list= S_Lnkd_Lst()#instantiate Class len(lcl_list) lcl_list.head=Node('Monday')#Add Head node lcl_list.head.nxt_node=Node('Tuesday')# Add Node to head lcl_list.head.nxt_node.nxt_node=Node('Thursday') lcl_list.head.nxt_node.nxt_node.nxt_node=Node('Friday') # add Node Sunday at head lcl_list.add_data('head','Sunday') lcl_list.dsply_lst() # add Node Saturday at tail lcl_list.add_data('tail','Saturday') lcl_list.dsply_lst() # add node wednesday after tuesday node lcl_list.add_data(lcl_list.head.nxt_node.nxt_node,'Wednesday') lcl_list.dsply_lst()","title":"Linked List"},{"location":"Python/Linked_list/#linked-list","text":"Create Node class with variables data and next_node Create Linked list Class, with variable name header. Create some node object and assign it to linked list head value. Create following functions. INsert Node at Head Insert Node at Tail Insert Node at some position Delete Node at head and tail Get length ofLinked List class Node: def __init__(self,data): self.data=data self.nxt_node = None class S_Lnkd_Lst: def __init__(self,): self.head = None def __len__(self): lcl_len=0 lcl_node=self.head while lcl_node is not None: lcl_len+=1 lcl_node=lcl_node.nxt_node return lcl_len def dsply_lst(self): tempnode=self.head while tempnode is not None: print(tempnode.data) tempnode=tempnode.nxt_node def add_data(self,position,data): lcl_node=Node(data) if position == 'head' : lcl_node.nxt_node = self.head self.head=lcl_node elif position == 'tail': if self.head is None: self.head=lcl_node return temp_node=self.head while temp_node.nxt_node is not None: temp_node=temp_node.nxt_node temp_node.nxt_node=lcl_node elif isinstance(position,Node): lcl_node.nxt_node=position.nxt_node position.nxt_node = lcl_node elif isinstance(position,int):# insert at some position if position > len(self) : print('CAnnot be insterted at {}',position) else: print('nothing') def rmv_node(self,position): if position is 'head': self.head=self.head.nxt_node if position is 'tail': if self.head is None: print('nothing to remove') return if len(self)==1: self.head=None return temp_node = self.head while temp_node.nxt_node is not None: prev=temp_node temp_node=temp_node.nxt_node prev.nxt_node=None lcl_list= S_Lnkd_Lst()#instantiate Class len(lcl_list) lcl_list.head=Node('Monday')#Add Head node lcl_list.head.nxt_node=Node('Tuesday')# Add Node to head lcl_list.head.nxt_node.nxt_node=Node('Thursday') lcl_list.head.nxt_node.nxt_node.nxt_node=Node('Friday') # add Node Sunday at head lcl_list.add_data('head','Sunday') lcl_list.dsply_lst() # add Node Saturday at tail lcl_list.add_data('tail','Saturday') lcl_list.dsply_lst() # add node wednesday after tuesday node lcl_list.add_data(lcl_list.head.nxt_node.nxt_node,'Wednesday') lcl_list.dsply_lst()","title":"Linked List"}]}