{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to MkDocs For full documentation visit mkdocs.org . Commands mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit. Project layout mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Welcome to MkDocs"},{"location":"#welcome-to-mkdocs","text":"For full documentation visit mkdocs.org .","title":"Welcome to MkDocs"},{"location":"#commands","text":"mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit.","title":"Commands"},{"location":"#project-layout","text":"mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Project layout"},{"location":"NLP/DeepLearning/","text":"Introduction Natural Language processing applications do not follow the ssumption of independently and identically distributed (i.i.d.). For example, the words in sentence are arranged in a meaningful way. If we disturb the order the meaning changes, Why? each word is dependent on the previous word. So we need a different architecture that captures temporal structure in sentence. The same is case with the time series data, where each data point is also a function of its previous satapoint. we shall look at various instances architecture like RNN(Recuurent neural network), LSTM(Long Short Term Memory) and GRU (Gated recurrent Unit) that captures temporal context for a time series problem. RNN Like Convolutional Neural Networks share parameters across space, RNNs share parameters over time. The following example predicting next character based on previous character is demonstrated in below images. So for every input x_t we are calculating context h_t and output computed based on h_t and h_t-1 , where h_t-1 is computed from previous observatopn x_t-1 . So variable h holds the context of the sequence. An RNN function is implemented as {h_t=f(x_t,h_t-1)} h_t= \\phi(Ux + Vh_t-1 + b_h) \\hat{y}= g(Wh_t + b_y) Where U,V and W are the weight matrices to compute conetxt variable h and output y . Pytorch RNN cell Parameters - * input_size \u2013 The number of expected features in the input x. * hidden_size \u2013 The number of features in the hidden state h. * num_layers \u2013 Number of recurrent layers. E.g., setting num_layers=2 would mean stacking two RNNs together to form a stacked RNN, with the second RNN taking in outputs of the first RNN and computing the final results. Default: 1 Inputs : input, h_0 * input : tensor of shape (L, N, H_{in} ) when batch_first=False or (N, L, H_{in} ) when batch_first=True containing the features of the input sequence. * h_0 : tensor of shape (D * \\text{num\\_layers}, N, H_{out}) containing the initial hidden state for each element in the batch. Defaults to zeros if not provided. Where N= batch size \\\\ L= sequence length \\\\ D= 2\\ if\\ bidirectional\\ =\\ True\\ otherwise\\ 1 \\\\ H_{in} = \\text{input\\_size} \\\\ H_{out} = \\text{hidden\\_size} \u200b Outputs : output, h_n * output : tensor of shape (L, N, D H_{out} ) when batch_first=False or (N, L, D*H_{out} ) when batch_first=True containing the output features (h_t) from the last layer of the RNN, for each t. h_n: tensor of shape (D * \\text{num\\_layers}, N, H_{out}) containing the final hidden state for each element in the batch. import torch from torch import nn rnn = nn.RNN(3,4) print(torch.randn(3,5)) input=torch.randn(5,2,3) print(input) output,final_hidden=rnn(input) print(output.shape,final_hidden.shape) # input is (5,2,3) # rnn cell is nn.RNN(3,4) # output is (5,2,4) & final hidden activation is (1,2,4) Back Propogation through time LSTM GRU Time Series problem References https://d2l.ai/chapter_recurrent-neural-networks/index.html https://deeplearning.neuromatch.io/tutorials/W2D3_ModernRecurrentNeuralNetworks/student/W2D3_Tutorial2.html https://pytorch.org/docs/stable/generated/torch.nn.RNN.html","title":"Introduction"},{"location":"NLP/DeepLearning/#introduction","text":"Natural Language processing applications do not follow the ssumption of independently and identically distributed (i.i.d.). For example, the words in sentence are arranged in a meaningful way. If we disturb the order the meaning changes, Why? each word is dependent on the previous word. So we need a different architecture that captures temporal structure in sentence. The same is case with the time series data, where each data point is also a function of its previous satapoint. we shall look at various instances architecture like RNN(Recuurent neural network), LSTM(Long Short Term Memory) and GRU (Gated recurrent Unit) that captures temporal context for a time series problem.","title":"Introduction"},{"location":"NLP/DeepLearning/#rnn","text":"Like Convolutional Neural Networks share parameters across space, RNNs share parameters over time. The following example predicting next character based on previous character is demonstrated in below images. So for every input x_t we are calculating context h_t and output computed based on h_t and h_t-1 , where h_t-1 is computed from previous observatopn x_t-1 . So variable h holds the context of the sequence. An RNN function is implemented as {h_t=f(x_t,h_t-1)} h_t= \\phi(Ux + Vh_t-1 + b_h) \\hat{y}= g(Wh_t + b_y) Where U,V and W are the weight matrices to compute conetxt variable h and output y .","title":"RNN"},{"location":"NLP/DeepLearning/#pytorch-rnn-cell","text":"Parameters - * input_size \u2013 The number of expected features in the input x. * hidden_size \u2013 The number of features in the hidden state h. * num_layers \u2013 Number of recurrent layers. E.g., setting num_layers=2 would mean stacking two RNNs together to form a stacked RNN, with the second RNN taking in outputs of the first RNN and computing the final results. Default: 1 Inputs : input, h_0 * input : tensor of shape (L, N, H_{in} ) when batch_first=False or (N, L, H_{in} ) when batch_first=True containing the features of the input sequence. * h_0 : tensor of shape (D * \\text{num\\_layers}, N, H_{out}) containing the initial hidden state for each element in the batch. Defaults to zeros if not provided. Where N= batch size \\\\ L= sequence length \\\\ D= 2\\ if\\ bidirectional\\ =\\ True\\ otherwise\\ 1 \\\\ H_{in} = \\text{input\\_size} \\\\ H_{out} = \\text{hidden\\_size} \u200b Outputs : output, h_n * output : tensor of shape (L, N, D H_{out} ) when batch_first=False or (N, L, D*H_{out} ) when batch_first=True containing the output features (h_t) from the last layer of the RNN, for each t. h_n: tensor of shape (D * \\text{num\\_layers}, N, H_{out}) containing the final hidden state for each element in the batch. import torch from torch import nn rnn = nn.RNN(3,4) print(torch.randn(3,5)) input=torch.randn(5,2,3) print(input) output,final_hidden=rnn(input) print(output.shape,final_hidden.shape) # input is (5,2,3) # rnn cell is nn.RNN(3,4) # output is (5,2,4) & final hidden activation is (1,2,4)","title":"Pytorch RNN cell"},{"location":"NLP/DeepLearning/#back-propogation-through-time","text":"","title":"Back Propogation through time"},{"location":"NLP/DeepLearning/#lstm","text":"","title":"LSTM"},{"location":"NLP/DeepLearning/#gru","text":"","title":"GRU"},{"location":"NLP/DeepLearning/#time-series-problem","text":"","title":"Time Series problem"},{"location":"NLP/DeepLearning/#references","text":"https://d2l.ai/chapter_recurrent-neural-networks/index.html https://deeplearning.neuromatch.io/tutorials/W2D3_ModernRecurrentNeuralNetworks/student/W2D3_Tutorial2.html https://pytorch.org/docs/stable/generated/torch.nn.RNN.html","title":"References"},{"location":"NLP/Encode_Text_Data/","text":"Encode Text Data Bag of Words Bag of words is method to represent raw text data in a strucutred format to a machine learning model. Bag of words method constructs Vocabulary from the given corpus, and represents the text data as a vecotor of size of the constructed vocabulary, with a number indicating the count of occurence of a word, that appeared in the sentecne. Example - 1. \"John likes to watch movies. Mary likes movies too.\" 2. \"Mary also likes to watch football games.\" Vocabulary - \"John\",\"likes\",\"to\",\"watch\",\"movies\",\"Mary\", \"too\", \"also\",\"football\",\"games\" Representation - \"John likes to watch movies. Mary likes movies too.\" is represented as [1, 2, 1, 1, 2, 1, 1, 0, 0, 0] \"Mary also likes to watch football games.\" is represented as [0, 1, 1, 1, 0, 1, 0, 1, 1, 1] Limitation - 1. As the vocabulary size increases, the lenght of vecotr increases, leading to increase in computational cost. 2. Context of the sentence is not captured, by ignoreing the word order Implementation - TF-IDF In bag of Words, we have seen that elements in the vecotr represents the number of times the word appeared in the sentence. Instead of count, TF-IDF is alternative way to construct vectors that yields better results. \"A problem with scoring word frequency is that highly frequent words start to dominate in the document (e.g. larger score), but may not contain as much \u201cinformational content\u201d to the model as rarer but perhaps domain specific words.\" (https://machinelearningmastery.com/gentle-introduction-bag-words-model/) Definition - Term Frequency: is a scoring of the frequency of the word in the current document. where ft,d is the raw count of a term in a document, i.e., the number of times that term t occurs in document d. Inverse Document Frequency: is a scoring of how rare the word is across documents. N: total number of documents in the corpus. {\\displaystyle N={|D|}}N = {|D|} {\\displaystyle |\\{d\\in D:t\\in d\\}|} |\\{d \\in D: t \\in d\\}| : number of documents where the term {\\displaystyle t}t appears (i.e., {\\displaystyle \\mathrm {tf} (t,d)\\neq 0} \\mathrm{tf}(t,d) \\neq 0). If the term is not in the corpus, this will lead to a division-by-zero. It is therefore common to adjust the denominator to {\\displaystyle 1+|\\{d\\in D:t\\in d\\}|}1 + |\\{d \\in D: t \\in d\\}|. Then tf\u2013idf is calculated as Example - References - 1. https://en.wikipedia.org/wiki/Bag-of-words_model 2. https://machinelearningmastery.com/gentle-introduction-bag-words-model/","title":"Encode Text Data"},{"location":"NLP/Encode_Text_Data/#encode-text-data","text":"","title":"Encode Text Data"},{"location":"NLP/Encode_Text_Data/#bag-of-words","text":"Bag of words is method to represent raw text data in a strucutred format to a machine learning model. Bag of words method constructs Vocabulary from the given corpus, and represents the text data as a vecotor of size of the constructed vocabulary, with a number indicating the count of occurence of a word, that appeared in the sentecne. Example - 1. \"John likes to watch movies. Mary likes movies too.\" 2. \"Mary also likes to watch football games.\" Vocabulary - \"John\",\"likes\",\"to\",\"watch\",\"movies\",\"Mary\", \"too\", \"also\",\"football\",\"games\" Representation - \"John likes to watch movies. Mary likes movies too.\" is represented as [1, 2, 1, 1, 2, 1, 1, 0, 0, 0] \"Mary also likes to watch football games.\" is represented as [0, 1, 1, 1, 0, 1, 0, 1, 1, 1] Limitation - 1. As the vocabulary size increases, the lenght of vecotr increases, leading to increase in computational cost. 2. Context of the sentence is not captured, by ignoreing the word order Implementation -","title":"Bag of Words"},{"location":"NLP/Encode_Text_Data/#tf-idf","text":"In bag of Words, we have seen that elements in the vecotr represents the number of times the word appeared in the sentence. Instead of count, TF-IDF is alternative way to construct vectors that yields better results. \"A problem with scoring word frequency is that highly frequent words start to dominate in the document (e.g. larger score), but may not contain as much \u201cinformational content\u201d to the model as rarer but perhaps domain specific words.\" (https://machinelearningmastery.com/gentle-introduction-bag-words-model/) Definition - Term Frequency: is a scoring of the frequency of the word in the current document. where ft,d is the raw count of a term in a document, i.e., the number of times that term t occurs in document d. Inverse Document Frequency: is a scoring of how rare the word is across documents. N: total number of documents in the corpus. {\\displaystyle N={|D|}}N = {|D|} {\\displaystyle |\\{d\\in D:t\\in d\\}|} |\\{d \\in D: t \\in d\\}| : number of documents where the term {\\displaystyle t}t appears (i.e., {\\displaystyle \\mathrm {tf} (t,d)\\neq 0} \\mathrm{tf}(t,d) \\neq 0). If the term is not in the corpus, this will lead to a division-by-zero. It is therefore common to adjust the denominator to {\\displaystyle 1+|\\{d\\in D:t\\in d\\}|}1 + |\\{d \\in D: t \\in d\\}|. Then tf\u2013idf is calculated as Example - References - 1. https://en.wikipedia.org/wiki/Bag-of-words_model 2. https://machinelearningmastery.com/gentle-introduction-bag-words-model/","title":"TF-IDF"},{"location":"Pre-Processing/Scaler/","text":"Scaling StandardScaler import pandas as pd from sklearn.preprocessing import StandardScaler scaler=StandardScaler() data = [i for i in range(1,31)] data.reshape(-1,2) x=pd.DataFrame(data) scaler.fit_transform(data) MinMaxScaler from sklearn.preprocessing import MinMaxScaler data = [[-1, 2], [-0.5, 6], [0, 10], [1, 18]] scaler = MinMaxScaler() print(scaler.fit(data)) MinMaxScaler() print(scaler.data_max_) [ 1. 18.] print(scaler.transform(data)) [[0. 0. ] [0.25 0.25] [0.5 0.5 ] [1. 1. ]] print(scaler.transform([[2, 2]])) [[1.5 0. ]] MaxAbsScaler from sklearn.preprocessing import MaxAbsScaler X = [[ 1., -1., 2.], [ 2., 0., 0.], [ 0., 1., -1.]] transformer = MaxAbsScaler().fit(X) transformer MaxAbsScaler() `transformer.transform(X)y array([[ 0.5, -1. , 1. ], [ 1. , 0. , 0. ], [ 0. , 1. , -0.5]]) RobustScaler","title":"Scaling"},{"location":"Pre-Processing/Scaler/#scaling","text":"","title":"Scaling"},{"location":"Pre-Processing/Scaler/#standardscaler","text":"import pandas as pd from sklearn.preprocessing import StandardScaler scaler=StandardScaler() data = [i for i in range(1,31)] data.reshape(-1,2) x=pd.DataFrame(data) scaler.fit_transform(data)","title":"StandardScaler"},{"location":"Pre-Processing/Scaler/#minmaxscaler","text":"from sklearn.preprocessing import MinMaxScaler data = [[-1, 2], [-0.5, 6], [0, 10], [1, 18]] scaler = MinMaxScaler() print(scaler.fit(data)) MinMaxScaler() print(scaler.data_max_) [ 1. 18.] print(scaler.transform(data)) [[0. 0. ] [0.25 0.25] [0.5 0.5 ] [1. 1. ]] print(scaler.transform([[2, 2]])) [[1.5 0. ]]","title":"MinMaxScaler"},{"location":"Pre-Processing/Scaler/#maxabsscaler","text":"from sklearn.preprocessing import MaxAbsScaler X = [[ 1., -1., 2.], [ 2., 0., 0.], [ 0., 1., -1.]] transformer = MaxAbsScaler().fit(X) transformer MaxAbsScaler() `transformer.transform(X)y array([[ 0.5, -1. , 1. ], [ 1. , 0. , 0. ], [ 0. , 1. , -0.5]])","title":"MaxAbsScaler"},{"location":"Pre-Processing/Scaler/#robustscaler","text":"","title":"RobustScaler"}]}